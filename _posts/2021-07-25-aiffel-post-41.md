---
title: "Lecture 5 | Convolutional Neural Networks"
excerpt: "Stanford University School of Engineering lecture summary"
categories:
  - Aiffel-DeepML
tags:
  - DeepML
toc: True
toc_sticky: True
last_modified_at: 2021-07-25T17:06:00Z
---


# Convolutional Neural Networks for Visual Recognition

## What do we want?

![image](https://user-images.githubusercontent.com/46912607/126897292-0fb57c8a-0aa4-4d6d-a893-ae939ac80dd2.png)

In the previous lecture, we covered how we can calculate the scores functions, SVM Loss and data loss + regularization. 

Now we want to find the parameters W that corresponds to our lowest loss.

**Why?** 

It is because we want to minimize the loss function, because we have a preference for simpler models for better generalization. 

We can achieve this with **optimiztation** 

We also can compute this gradient with

- numerical gradient method


    - slow, approximate, easy to write
    
    
- analytic gradient 
    
    
    - fast, exact, error-prone
    
In practice, derive analytic gradient then check the implementation with numerical gradient 

## Computational graphs


![image](https://user-images.githubusercontent.com/46912607/126897694-eeab621b-b436-4549-8375-7f3a9877e3db.png)
    
**What is a computation graph?**

We can use this kind of graph in order to represent any function where the nodes of the graph are steps of computation that we go through. 

노드의 연산 단계를 나타냅니다. 

이 예제는 linear 선형 classifier 입니다. 

inputs: X and W 

multiplication node: represents the matrix multiplier (행렬 곱셈)

vector of scores: multiplication of parameters W and data X

[hinge loss](https://www.notion.so/modulabs/Lec03-1-Loss-Function-5e775d9c809a433eb243956d23bd379b#2839e4efd7854a9399f56e0f0fec5bb2): data loss term. 

Total Loss: Sum of regularization term and the data term 

**Advantage** 

we can call back propagation! 

- gradient를 얻기위해 computational graph 내부의 모든 변수에 대해 chain rule을 재귀적으로 사용합니다

- really useful wehen working with complex functions 

    - Convolutional network (AlexNet)
    
    - Neural Turing Machine

이걸 직접 계산하는 거는 정신나간 짓이다.

## Back propogation 

### example1

![image](https://user-images.githubusercontent.com/46912607/126898426-7058860b-1117-46ca-b72e-1438c0f574f3.png)

backpropagation은 chain rule의 재귀적인 응용입니다.
chiain rule에 의해 우리는 뒤에서부터 시작하기 때문에 뒤에서 부터 gradient 계산을 합니다. 

y 와 f 는 직접 연결 되어 있지 않아서 chain rule을 사용한다. 

y에 대한 f의 미분은 q에 대한 f의 미분과
y에 대한 q의 미분의 곱으로 나타낼 수 있습니다.

### example2

![image](https://user-images.githubusercontent.com/46912607/126898685-9e9356ff-983a-45d9-9355-d424b82a9eb6.png)

If we take a loot at what we did in a different perspective as nodes, we see that we have the L(LOSS) value coming back as back propogation. We use the chain rule to multiply hte local gradient and upstream gradient coming down in order to get the gradient respect to the input. 

### example3

![image](https://user-images.githubusercontent.com/46912607/126899340-e3d2141d-021f-4d71-8f8e-017538a9ed4b.png)

We define the computational nodes into any granularity we want to. 

In practice, we can group some of the nodes together as long as we can write down the local gradient for the function. 

For example, we can use the sigmoid function to shorten the nodes. 

**Trade off**

how much math for simpler graphs vs how simple you want your gradients to be

## patterns in backward flow

![image](https://user-images.githubusercontent.com/46912607/126899918-fd0b8475-0d0b-4106-ac35-5035bc017d70.png)

## Gradients for vectorized code

The equation would stay the same with the only difference being that this is now a jocobian matrix. derivative of each element z with respect to derivative of each element x. 

![image](https://user-images.githubusercontent.com/46912607/126900196-c4d71dcf-c2b7-4476-a58c-9df2638751a6.png)


![image](https://user-images.githubusercontent.com/46912607/126900725-9625470b-c731-4da2-bafc-ff036ea298c3.png)

## implementation

![image](https://user-images.githubusercontent.com/46912607/126900822-2c3cd2e5-6a58-4a88-be37-1f8d4a677ad4.png)



![image](https://user-images.githubusercontent.com/46912607/126900831-3a57a332-12e1-41e6-bcd6-e49a2d39fd2d.png)



![image](https://user-images.githubusercontent.com/46912607/126900835-2f6d2f99-5956-4dfb-854a-cef56cb49750.png)

## summary 

- neural nets will be very large: impractical to write down gradient formula by hand for all parameters

- backpropagation = recursive application of the chain rule along a computational graph to compute the gradients of all inputs/parameters/intermediates

- implementations maintain a graph structure, where the nodes implement the forward() / backward() API

- forward: compute result of an operation and save any intermediates needed for gradient computation in memory

- backward: apply the chain rule to compute the gradient of the loss function with respect to the inputs

# neural networks

![image](https://user-images.githubusercontent.com/46912607/126900907-a42cacd9-f994-4824-bf45-6317357e4b7f.png)

## activate functions

![image](https://user-images.githubusercontent.com/46912607/126901340-bed48da4-33ce-4784-a86d-e403fd8a1428.png)

## summary 

- We arrange neurons into fully-connected layers

- The abstraction of a layer has the nice property that it allows us to use efficient vectorized code (e.g. matrix multiplies)

- Neural networks are not really neural

- Next time: Convolutional Neural Networks